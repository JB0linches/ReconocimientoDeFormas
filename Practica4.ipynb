{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "AfaNEPeRXCe8"
   },
   "source": [
    "# Práctica 1 Reconocimiento de Formas: Clasificador de la distancia euclídea \n",
    "\n",
    "* **Alumno 1**: Bolinches Segovia, Jorge\n",
    "* **Alumno 2**: Cercadillo Muñoz, Daniel\n",
    "* **Alumno 3**: Cerezo Pomykol, Jan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FsWqPK6l2flD"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from abc import abstractmethod\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def decision_function(self, X):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.num_aciertos(X, y)/len(y)\n",
    "\n",
    "\n",
    "class ClassifEuclid(Classifier, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, labels=[]):\n",
    "        \"\"\"Constructor de la clase\n",
    "        labels: lista de etiquetas de esta clase (argumento necesario)\"\"\"\n",
    "        self.labels = labels\n",
    "        self.Z = None # Array de centroides\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrena el clasificador\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        y: vector de etiquetas, tantos elementos como filas en X\n",
    "        retorna objeto clasificador\"\"\"\n",
    "        n = np.zeros(len(self.labels)) # Contador de ocurrencias de cada clase\n",
    "        self.Z = np.zeros((len(self.labels), X.shape[1]))\n",
    "        # Calcular la media: \n",
    "        # Sumar las ocurrencias de cada clase en self.Z\n",
    "        for yi, Xi in zip(y, X):\n",
    "            n[yi] = n[yi] + 1\n",
    "            self.Z[yi] = self.Z[yi] + Xi\n",
    "        # Dividir cada sumatorio entre el númeo de ocurrencias\n",
    "        self.Z = self.Z / n[:,np.newaxis]\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases \n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas. \n",
    "        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n",
    "        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n",
    "        # Calcular la distancia de cada fila a cada centroide\n",
    "        aux = X[:,None]-self.Z\n",
    "        return np.sqrt(np.einsum('abc,abc->ab', aux, aux))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        retorna un vector con las etiquetas de cada dato\"\"\"\n",
    "        # Devuelve un array con el índice con valor mínimo de cada fila.\n",
    "        # Cada índice se corresponde con la clase a la que pertenece.\n",
    "        return np.argmin(self.decision_function(X), axis=1)\n",
    "    \n",
    "    def num_aciertos(self, X, y):\n",
    "        \"\"\"Cuenta el numero de aciertos del clasificador para un conjunto de datos X.\n",
    "        X: matriz de datos a clasificar\n",
    "        y: vector de etiquetas correctas\"\"\"\n",
    "        # Contar el número de datos iguales en ambos vectores\n",
    "        return np.sum(self.predict(X)==y)\n",
    "\n",
    "    \n",
    "class ClassifEstadistico(Classifier, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, labels=[]):\n",
    "        \"\"\"Constructor de la clase\n",
    "        labels: lista de etiquetas de esta clase (argumento necesario)\"\"\"\n",
    "        self.labels = labels\n",
    "        self.mu = None # Array de medias\n",
    "        self.cov = None # Array de matrices de covarianza de cada clase\n",
    "        self.cov_inv = None # Array de matrices de covarianza inversas\n",
    "        self.det = None # Array de determinantes de las matrices de covarianza\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "        self.c = None\n",
    "        self.reg_param = None\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"Entrena el clasificador\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        y: vector de etiquetas, tantos elementos como filas en X\n",
    "        retorna objeto clasificador\"\"\"\n",
    "        n_labels = len(self.labels)\n",
    "        n_caracteristicas = X.shape[1]\n",
    "        self.mu = np.empty((n_labels, n_caracteristicas))\n",
    "        self.cov = np.empty((n_labels, n_caracteristicas, n_caracteristicas))\n",
    "        self.cov_inv = np.empty((n_labels, n_caracteristicas, n_caracteristicas))\n",
    "        self.det = np.empty(n_labels)\n",
    "        self.a = np.empty((n_labels, n_caracteristicas, n_caracteristicas))\n",
    "        self.b = np.empty((n_labels, n_caracteristicas))\n",
    "        self.c = np.empty(n_labels)\n",
    "        for c in range(len(self.labels)):\n",
    "            self.cov[c] = np.cov(X[y==c], rowvar=False)\n",
    "            self.mu[c] = np.mean(X[y==c], axis=0)\n",
    "            self.cov_inv[c] = np.linalg.inv(self.cov[c])\n",
    "            self.det[c] = np.linalg.det(self.cov[c])\n",
    "            self.a[c] = -.5 * self.cov_inv[c]\n",
    "            self.b[c] = self.mu[c].T @ self.cov_inv[c]\n",
    "            self.c[c] = -.5 * (self.mu[c].T @ self.cov_inv[c] @ self.mu[c]) -.5 * np.log(self.det[c]) + np.log(np.sum(y==c)/X.shape[0])\n",
    "        \n",
    "        # regulaarizado\n",
    "        # añadir reg_param a la clase\n",
    "        # calcular cada coso de la clase para cada valor de reg_param\n",
    "        # recaulcular cov:\n",
    "        # cov = (1 - reg_param) * cov + reg_param * np.eye(n_features)\n",
    "        # adaptar los otros metodos para admitir los reg_param\n",
    "        \n",
    "        #cov_media = np.mean(cov, axis=0)\n",
    "        return self\n",
    "\n",
    "    def decision_function(self,X):\n",
    "        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases \n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas. \n",
    "        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n",
    "        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n",
    "        return np.einsum('ab,cdb,ad->ac', X, self.a, X) + np.einsum('ab,cb->ca', self.b, X) + self.c[None,:]\n",
    "\n",
    "    def predict(self,X):\n",
    "        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        retorna un vector con las etiquetas de cada dato\"\"\"\n",
    "        # Devuelve un array con el índice con valor mínimo de cada fila.\n",
    "        # Cada índice se corresponde con la clase a la que pertenece.\n",
    "        return np.argmax(self.decision_function(X), axis=1)\n",
    "    \n",
    "    def num_aciertos(self,X,y):\n",
    "        \"\"\"Cuenta el numero de aciertos del clasificador para un conjunto de datos X.\n",
    "        X: matriz de datos a clasificar\n",
    "        y: vector de etiquetas correctas\"\"\"\n",
    "        # Contar el número de datos iguales en ambos vectores\n",
    "        return np.sum(self.predict(X)==y)\n",
    "\n",
    "\n",
    "class ClassifEstadisticoRegularizado(Classifier, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, labels=[], reg_param=0):\n",
    "        self.labels = labels\n",
    "        self.reg_param = reg_param # hiperparametro\n",
    "        self.mu = None # Array de medias\n",
    "        self.cov = None # Array de matrices de covarianza de cada clase\n",
    "        self.cov_reg = None # Array de matrices de covarianza regularizadas\n",
    "        self.cov_reg_inv = None # Array de matrices de covarianza inversas\n",
    "        self.det = None # Array de determinantes de las matrices de covarianza\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "        self.c = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_labels = len(self.labels)\n",
    "        n_caracteristicas = X.shape[1]\n",
    "        if self.mu is None: # si es la primera vez que se hace fit\n",
    "            self.mu = np.empty((n_labels, n_caracteristicas))\n",
    "            self.cov = np.empty((n_labels, n_caracteristicas, n_caracteristicas))\n",
    "            self.cov_reg = np.empty((n_labels, n_caracteristicas, n_caracteristicas))\n",
    "            self.cov_reg_inv = np.empty((n_labels, n_caracteristicas, n_caracteristicas))\n",
    "            self.det = np.empty(n_labels)\n",
    "            self.a = np.empty((n_labels, n_caracteristicas, n_caracteristicas))\n",
    "            self.b = np.empty((n_labels, n_caracteristicas))\n",
    "            self.c = np.empty(n_labels)\n",
    "        for c in range(len(self.labels)):\n",
    "            X_clase = X[y==c, :]\n",
    "            self.mu[c] = np.mean(X_clase, axis=0)\n",
    "            X_new = X_clase - self.mu[c]\n",
    "            self.cov[c] = np.cov(X_clase, rowvar=False)\n",
    "            # descomposicion espectral\n",
    "            #_, autovalores, autovectores = np.linalg.svd(X_new, full_matrices=False)\n",
    "            self.cov_reg[c] = (1 - self.reg_param) * self.cov[c] + self.reg_param * (np.trace(self.cov[c])/n_caracteristicas) * np.eye(n_caracteristicas)\n",
    "            #self.cov_reg[c] = (1 - self.reg_param) * self.cov[c] + self.reg_param * self.cov_mu\n",
    "            self.det[c] = np.linalg.det(self.cov_reg[c])\n",
    "            if self.det[c] > 1e-4:\n",
    "                self.cov_reg_inv[c] = np.linalg.inv(self.cov_reg[c])\n",
    "                self.a[c] = -.5 * self.cov_reg_inv[c]\n",
    "                self.b[c] = self.mu[c].T @ self.cov_reg_inv[c]\n",
    "                self.c[c] = -.5 * (self.mu[c].T @ self.cov_reg_inv[c] @ self.mu[c]) -.5 * np.log(self.det[c]) + np.log(np.sum(X_clase)/X.shape[0])\n",
    "            else:\n",
    "                self.a[c] = np.zeros((n_caracteristicas, n_caracteristicas))\n",
    "                self.b[c] = np.zeros(n_caracteristicas)\n",
    "                self.c[c] = 0\n",
    "        return self\n",
    "\n",
    "    def decision_function(self,X):\n",
    "        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases \n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas. \n",
    "        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n",
    "        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n",
    "        #return 0.5 * np.log(\n",
    "        return np.einsum('ab,cdb,ad->ac', X, self.a, X) + np.einsum('ab,cb->ca', self.b, X) + self.c[None, :]\n",
    "    def predict(self,X):\n",
    "        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        retorna un vector con las etiquetas de cada dato\"\"\"\n",
    "        # Devuelve un array con el índice con valor mínimo de cada fila.\n",
    "        # Cada índice se corresponde con la clase a la que pertenece.\n",
    "        return np.argmax(self.decision_function(X), axis=1)\n",
    "    \n",
    "    def num_aciertos(self,X,y):\n",
    "        \"\"\"Cuenta el numero de aciertos del clasificador para un conjunto de datos X.\n",
    "        X: matriz de datos a clasificar\n",
    "        y: vector de etiquetas correctas\"\"\"\n",
    "        # Contar el número de datos iguales en ambos vectores\n",
    "        return np.sum(self.predict(X)==y)\n",
    "    \n",
    "    \n",
    "class ExclusionSplitter:\n",
    "    \"\"\"Esta clase nos permite usar GridSearchCV con la valuación por exclusion.\"\"\"\n",
    "    def __init__(self, train_indices, test_indices):\n",
    "        self.train_indices = train_indices\n",
    "        self.test_indices = test_indices\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        return [(self.train_indices, self.test_indices)]\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.datasets import load_iris\\ndataset = load_iris()\\nX = dataset.data\\ny = dataset.target\\nreg_param = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\\nclf = ClassifEstadisticoRegularizado(dataset.target_names, reg_param)\\nclf.fit(X, y)\\n#clf.decision_function(X)\\nclf.num_aciertos(X, y)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "reg_param = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "clf = ClassifEstadisticoRegularizado(dataset.target_names, reg_param)\n",
    "clf.fit(X, y)\n",
    "#clf.decision_function(X)\n",
    "clf.num_aciertos(X, y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0U0Awu0X01F"
   },
   "source": [
    "# Entrenamiento, predicción y evaluación de iris, wine y cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xasWFNlhX85L",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################################\n",
      "\tClasificador distancia euclídea\n",
      "############################################################################\n",
      "\t--------------------------------------------------------------------\n",
      "\tiris:   Aciertos: 139/150 (92.67%)\n",
      "\t\tEvaluación por resustitución: 0.9267\n",
      "\t\tEvaluación por validación cruzada: 0.9133, std:0.0499\n",
      "\t\tClasificador sklearn: 139 aciertos\n",
      "\t--------------------------------------------------------------------\n",
      "\twine:   Aciertos: 129/178 (72.47%)\n",
      "\t\tEvaluación por resustitución: 0.7247\n",
      "\t\tEvaluación por validación cruzada: 0.7187, std:0.0804\n",
      "\t\tClasificador sklearn: 129 aciertos\n",
      "\t--------------------------------------------------------------------\n",
      "\tcancer: Aciertos: 507/569 (89.10%)\n",
      "\t\tEvaluación por resustitución: 0.8910\n",
      "\t\tEvaluación por validación cruzada: 0.8841, std:0.0840\n",
      "\t\tClasificador sklearn: 507 aciertos\n",
      "\n",
      "\n",
      "############################################################################\n",
      "\tClasificador estadístico\n",
      "############################################################################\n",
      "\t--------------------------------------------------------------------\n",
      "\tiris:   Aciertos: 147/150 (98.00%)\n",
      "\t\tEvaluación por resustitución: 0.9800\n",
      "\t\tEvaluación por validación cruzada: 0.9600, std:0.0646\n",
      "\t\tClasificador sklearn: 147 aciertos\n",
      "\t--------------------------------------------------------------------\n",
      "\twine:   Aciertos: 177/178 (99.44%)\n",
      "\t\tEvaluación por resustitución: 0.9944\n",
      "\t\tEvaluación por validación cruzada: 0.7108, std:0.3660\n",
      "\t\tClasificador sklearn: 177 aciertos\n",
      "\t--------------------------------------------------------------------\n",
      "\tcancer: Aciertos: 554/569 (97.36%)\n",
      "\t\tEvaluación por resustitución: 0.9736\n",
      "\t\tEvaluación por validación cruzada: 0.9613, std:0.0090\n",
      "\t\tClasificador sklearn: 554 aciertos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\"\"\"\n",
    "Clasificador distancia euclídea\n",
    "\"\"\"\n",
    "print(\"#\"*76, \"\\tClasificador distancia euclídea\", \"#\"*76, sep=\"\\n\")\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "a = ClassifEuclid(dataset.target_names)\n",
    "\n",
    "# 2. Entrenar el clasificador\n",
    "train = a.fit(np.array(X), y)\n",
    "\n",
    "# 3. Predecir y evaluar el clasificador calculando el porcentaje de datos correctamente clasificados\n",
    "n_aciertos = a.num_aciertos(X, y)\n",
    "print(\"\\t\", \"-\"*68, \"\\n\\tiris:   Aciertos: \", n_aciertos, \"/\", len(y), \" (\", \"%.2f\" % ((n_aciertos / len(y))*100), \"%)\", sep='')\n",
    "\n",
    "# Evaluación por resustitución y validación cruzada\n",
    "print(\"\\t\\tEvaluación por resustitución:\", \"%.4f\" % a.score(X,y))\n",
    "scores = cross_val_score(a, X, y, cv=5)\n",
    "print(\"\\t\\tEvaluación por validación cruzada: \", \"%.4f\" % np.mean(scores), \", std:\", \"%.4f\" % np.std(scores), sep='')\n",
    "\n",
    "# Comparación con el clasificador de sklearn\n",
    "nc = NearestCentroid()\n",
    "nc.fit(X, y)\n",
    "print(\"\\t\\t\", \"Clasificador sklearn: \", np.sum(nc.predict(X)==y), \" aciertos\", sep='')\n",
    "\n",
    "\n",
    "# wine\n",
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "b = ClassifEuclid(dataset.target_names)\n",
    "train = b.fit(np.array(X), y)\n",
    "n_aciertos = b.num_aciertos(X, y)\n",
    "print(\"\\t\", \"-\"*68, \"\\n\\twine:   Aciertos: \", n_aciertos, \"/\", len(y), \" (\", \"%.2f\" % ((n_aciertos / len(y))*100), \"%)\", sep='')\n",
    "print(\"\\t\\tEvaluación por resustitución:\", \"%.4f\" % b.score(X,y))\n",
    "scores = cross_val_score(b, X, y, cv=5)\n",
    "print(\"\\t\\tEvaluación por validación cruzada: \", \"%.4f\" % np.mean(scores), \", std:\", \"%.4f\" % np.std(scores), sep='')\n",
    "nc = NearestCentroid()\n",
    "nc.fit(X, y)\n",
    "print(\"\\t\\t\", \"Clasificador sklearn: \", np.sum(nc.predict(X)==y), \" aciertos\", sep='')\n",
    "\n",
    "\n",
    "# cancer\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "c = ClassifEuclid(dataset.target_names)\n",
    "train = c.fit(np.array(X), y)\n",
    "n_aciertos = c.num_aciertos(X, y)\n",
    "print(\"\\t\", \"-\"*68, \"\\n\\tcancer: Aciertos: \", n_aciertos, \"/\", len(y), \" (\", \"%.2f\" % ((n_aciertos / len(y))*100), \"%)\", sep='')\n",
    "print(\"\\t\\tEvaluación por resustitución:\", \"%.4f\" % c.score(X,y))\n",
    "scores = cross_val_score(c, X, y, cv=5)\n",
    "print(\"\\t\\tEvaluación por validación cruzada: \", \"%.4f\" % np.mean(scores), \", std:\", \"%.4f\" % np.std(scores), sep='')\n",
    "nc = NearestCentroid()\n",
    "nc.fit(X, y)\n",
    "print(\"\\t\\t\", \"Clasificador sklearn: \", np.sum(nc.predict(X)==y), \" aciertos\", sep='')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Clasificador estadístico\n",
    "\"\"\"\n",
    "print(\"\\n\", \"#\"*76, \"\\tClasificador estadístico\", \"#\"*76, sep=\"\\n\")\n",
    "# iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "b = ClassifEstadistico(dataset.target_names)\n",
    "train = b.fit(np.array(X), y)\n",
    "n_aciertos = b.num_aciertos(X, y)\n",
    "print(\"\\t\", \"-\"*68, \"\\n\\tiris:   Aciertos: \", n_aciertos, \"/\", len(y), \" (\", \"%.2f\" % ((n_aciertos / len(y))*100), \"%)\", sep='')\n",
    "print(\"\\t\\tEvaluación por resustitución:\", \"%.4f\" % b.score(X,y))\n",
    "scores = cross_val_score(b, X, y, cv=5)\n",
    "print(\"\\t\\tEvaluación por validación cruzada: \", \"%.4f\" % np.mean(scores), \", std:\", \"%.4f\" % np.std(scores), sep='')\n",
    "c_est = QuadraticDiscriminantAnalysis()\n",
    "c_est.fit(X, y)\n",
    "print(\"\\t\\t\", \"Clasificador sklearn: \", np.sum(c_est.predict(X)==y), \" aciertos\", sep='')\n",
    "\n",
    "# wine\n",
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "b = ClassifEstadistico(dataset.target_names)\n",
    "train = b.fit(np.array(X), y)\n",
    "n_aciertos = b.num_aciertos(X, y)\n",
    "print(\"\\t\", \"-\"*68, \"\\n\\twine:   Aciertos: \", n_aciertos, \"/\", len(y), \" (\", \"%.2f\" % ((n_aciertos / len(y))*100), \"%)\", sep='')\n",
    "print(\"\\t\\tEvaluación por resustitución:\", \"%.4f\" % b.score(X,y))\n",
    "scores = cross_val_score(b, X, y, cv=5)\n",
    "print(\"\\t\\tEvaluación por validación cruzada: \", \"%.4f\" % np.mean(scores), \", std:\", \"%.4f\" % np.std(scores), sep='')\n",
    "c_est = QuadraticDiscriminantAnalysis()\n",
    "c_est.fit(X, y)\n",
    "print(\"\\t\\t\", \"Clasificador sklearn: \", np.sum(c_est.predict(X)==y), \" aciertos\", sep='')\n",
    "\n",
    "# cancer\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "b = ClassifEstadistico(dataset.target_names)\n",
    "train = b.fit(np.array(X), y)\n",
    "n_aciertos = b.num_aciertos(X, y)\n",
    "print(\"\\t\", \"-\"*68, \"\\n\\tcancer: Aciertos: \", n_aciertos, \"/\", len(y), \" (\", \"%.2f\" % ((n_aciertos / len(y))*100), \"%)\", sep='')\n",
    "print(\"\\t\\tEvaluación por resustitución:\", \"%.4f\" % b.score(X,y))\n",
    "scores = cross_val_score(b, X, y, cv=5)\n",
    "print(\"\\t\\tEvaluación por validación cruzada: \", \"%.4f\" % np.mean(scores), \", std:\", \"%.4f\" % np.std(scores), sep='')\n",
    "c_est = QuadraticDiscriminantAnalysis()\n",
    "c_est.fit(X, y)\n",
    "print(\"\\t\\t\", \"Clasificador sklearn: \", np.sum(c_est.predict(X)==y), \" aciertos\", sep='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Resultados de los tres experimentos (clasificador distancia euclídea):\n",
    "\n",
    "| Base de datos | Número de aciertos | Porcentaje de aciertos |\n",
    "| --- | --- | --- |\n",
    "| Iris   | 139| 92.67|\n",
    "| Wine   | 129| 72.47|\n",
    "| Cancer | 507| 89.10|\n",
    "\n",
    "Resultados de los tres experimentos (clasificador estadístico):\n",
    "\n",
    "| Base de datos | Número de aciertos | Porcentaje de aciertos |\n",
    "| --- | --- | --- |\n",
    "| Iris   | 147| 98.00|\n",
    "| Wine   | 177| 99.44|\n",
    "| Cancer | 554| 97.37|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y evaluación de Isolet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador distancia euclídea:\n",
      "\tEvaluación por resustitución: 0.8809\n",
      "\tEvaluación por exclusión: 0.8743\n",
      "Clasificador estadístico:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Si existe la base de datos, cargo las variables\n",
    "if os.path.exists(\"isolet_X.pickle\"):\n",
    "    X = pd.read_pickle('isolet_X.pickle')\n",
    "    y = pd.read_pickle('isolet_y.pickle')\n",
    "else:\n",
    "    # Cargamos desde internet ( https://www.openml.org ) y la guardamos en el directorio local\n",
    "    X, y = fetch_openml('isolet', version=1, return_X_y=True, cache=False)\n",
    "    # Guardamos los datos para no volver a descargarlos\n",
    "    X.to_pickle(\"isolet_X.pickle\")\n",
    "    y.to_pickle(\"isolet_y.pickle\")\n",
    "\n",
    "X_train = np.array(X[:6238])\n",
    "y_train = pd.factorize(y)[0][:6238]\n",
    "X_test = np.array(X[6238:])\n",
    "y_test = pd.factorize(y)[0][6238:]\n",
    "\n",
    "X = np.array(X)\n",
    "y = pd.factorize(y)[0]\n",
    "\n",
    "# Clasificador distancia euclidea\n",
    "clss_euc = ClassifEuclid(np.unique(y_train))\n",
    "clss_euc.fit(X_train, y_train)\n",
    "# Clasificador estadístico\n",
    "\"\"\"\n",
    "clss_est = QuadraticDiscriminantAnalysis()\n",
    "clss_est.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "\n",
    "clss_est = ClassifEstadisticoRegularizado(np.unique(y_train))\n",
    "clss_est.fit(X_train, y_train)\n",
    "\n",
    "print(\"Clasificador distancia euclídea:\")\n",
    "print(\"\\tEvaluación por resustitución:\", \"%.4f\" % clss_euc.score(X_train, y_train))\n",
    "print(\"\\tEvaluación por exclusión:\", \"%.4f\" % clss_euc.score(X_test, y_test))\n",
    "print(\"Clasificador estadístico:\")\n",
    "print(\"\\tEvaluación por resustitución:\", \"%.4f\" % clss_est.score(X_train, y_train))\n",
    "print(\"\\tEvaluación por exclusión:\", \"%.4f\" % clss_est.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados de los experimentos con el clasificador de la distancia euclídea:\n",
    "\n",
    "| Base de datos | Acierto resustitucion | Acierto validacion cruzada | Acierto exclusion |\n",
    "| --- | --- | --- | --- |\n",
    "| Iris   | 0.9267 | 0.9133 | - |\n",
    "| Wine   | 0.7247 | 0.7187 | - |\n",
    "| Cancer | 0.8910 | 0.8841 | - |\n",
    "| Isolet |  0.8809 | - | 0.8743 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados de los experimentos con el clasificador estadístico bayesiano:\n",
    "\n",
    "| Base de datos | Acierto resustitucion | Acierto validacion cruzada | Acierto exclusion |\n",
    "| --- | --- | --- | --- |\n",
    "| Iris   | 0.9800 | 0.9600 | - |\n",
    "| Wine   | 0.9944 | 0.7108 | - |\n",
    "| Cancer | 0.9736 | 0.9613 | - |\n",
    "| Isolet |  1.0000 | - | 0.0648 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentarios sobre los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijándonos en la columna de aciertos por sustitución, vemos que el estadístico comete menos errores en las 3 bases de datos,\n",
    "siendo mas destacable la diferencia en cuanto al porcentaje de aciertos en las de Wine y Cancer, puesto que pasa de un 72,47%\n",
    "a un 99,44% en Wine y de un 89,1% a un 97,36%. Sin embargo, este incremento de la tasa de aciertos no se corresponde con un\n",
    "aumento real de la precisión del clasificador, si no con un error en la forma de evaluar el rendimiento de los clasificadores.\n",
    "Al evaluar ambos por validación cruzada, comprobamos que el aumento no es tan grande como el obtenido evaluando solo por\n",
    "resusitución, siendo incluso menor la precisión del estadístico en Wine a pesar de que era donde a priori se producia el mayor\n",
    "aumento. A pesar del peor rendimiento obtenido por el bayesiano en Wine, no podemos obviar que ha desempeñado mejor su funcion\n",
    "en las otras dos bases de datos, mostrando incluso un aumento de la tasas de aciertos desde un 88,41% a un 96,13% en Cancer.\n",
    "\n",
    "En cuanto a los resultados obtenidos en Isolet, vemos que los resultados del euclideo concuerdan más con los resultados que se\n",
    "esperan de un clasificador que los del bayesiano. Como podemos ver, en el bayesiano se ha obtenido un 100% de precision por\n",
    "resustitución, sin embargo se ha obtenido un 6,48% por exclusion. Este fenómeno se debe a la existencia de variables linealmente\n",
    "dependientes en el conjunto de entrenamiento de isolet. Debido a las variables colineares, es imposible hallar la inversa de la\n",
    "matriz de covarianzas, por lo que estos datos no se tienen en cuenta afectando negativamente al rendimiento del bayesiano, sin\n",
    "embargo esto no sucede con el euclideo puesto que no tiene en cuenta el factor de la dispersion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\discriminant_analysis.py:873: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSelected shrinkage = 0.23\n",
      "\tAccuracy: 0.956 (+/- 0.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\AppData\\Local\\Temp/ipykernel_1800/1483422701.py:176: RuntimeWarning: divide by zero encountered in log\n",
      "  self.c[r][c] = -.5 * (self.mu[c].T @ self.cov_inv[r][c] @ self.mu[c]) -.5 * np.log(self.det[r][c]) + np.log(np.sum(y==c)/X.shape[0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1800/232067362.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m#clf.decision_function(X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_aciertos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1800/1483422701.py\u001b[0m in \u001b[0;36mnum_aciertos\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;31m# Contar el número de datos iguales en ambos vectores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1800/1483422701.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;31m# Devuelve un array con el índice con valor mínimo de cada fila.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;31m# Cada índice se corresponde con la clase a la que pertenece.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnum_aciertos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1800/1483422701.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ab,cdb,ad->ac'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ab,cb->ca'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m#return np.einsum('ab,cdb,ad->ac', X, self.a, X) + np.einsum('ab,cb->ca', self.b, X) + self.c[None, :]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\core\\einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1357\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspecified_out\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;31m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def find_best_parameters_quad(X, y, k, shrinkages):\n",
    "    \"\"\"\n",
    "    Busca el clasificador bayesiano regularizado con el mejores parámetros.\n",
    "    :param X: Ejemplos de la dase de datos\n",
    "    :param y: Etiquetas de los ejemplos\n",
    "    :param k: Número de divisiones en la validación cruzada (k-fold)\n",
    "    :param shrinkages: Lista de posibles valores del parametro de shrinkage que conforman la rejilla\n",
    "    \"\"\"\n",
    "\n",
    "    # Creamos una instancia del clasificador\n",
    "    cbp = QuadraticDiscriminantAnalysis()\n",
    "    # definimos la rejilla en la que vamos a buscar\n",
    "    params = {'reg_param': shrinkages}\n",
    "    # Creamos una clase GridSearchCV que será una especie de supra-clasificador\n",
    "    # que se ajusta por validación cruzada.\n",
    "    clf = GridSearchCV(cbp, params, n_jobs=-1, scoring='accuracy', cv=k).fit(X, y)\n",
    "    # Como supra-clasificador que es, clf contiene todo tipo de datos sobre la evaluación\n",
    "    # por validación cruzada. Vamos a obtener en este caso el clasificador con mejor 'accuracy'\n",
    "    best_clf = clf.best_estimator_\n",
    "    # clf.cv_results_ contiene los resultados de la evaluación, obtengamos la media y\n",
    "    # desviación típica del score de validación\n",
    "    result_score_mean = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "    result_score_std = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "    # print(\"Srinkage scores: \", clf.cv_results_['mean_test_score'])\n",
    "\n",
    "    # Imprimamos los mejores parametro que hemos encontrado y el resultado de su validación\n",
    "    print(\"\\tSelected shrinkage = {}\\n\" \\\n",
    "          \"\\tAccuracy: {:.3f} (+/- {:.3f})\".format(best_clf.reg_param,\n",
    "                                                   result_score_mean,\n",
    "                                                   result_score_std))\n",
    "    return best_clf\n",
    "\n",
    "def find_best_parameters_quad2(X, y, k, shrinkages):\n",
    "        \n",
    "    \n",
    "    cbp = ClassifEstadistico(np.unique(y))\n",
    "    params = {'reg_param': shrinkages}\n",
    "    clf = GridSearchCV(cbp, params, n_jobs=-1, scoring='accuracy', cv=k).fit(X, y)\n",
    "    best_clf = clf.best_estimator_\n",
    "    result_score_mean = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "    result_score_std = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "\n",
    "    print(\"\\tSelected shrinkage = {}\\n\" \\\n",
    "          \"\\tAccuracy: {:.3f} (+/- {:.3f})\".format(best_clf.reg_param,\n",
    "                                                   result_score_mean,\n",
    "                                                   result_score_std))\n",
    "    return best_clf\n",
    "\n",
    "\n",
    "#ExclusionSplitter(np.arange(0, len(X_train)), np.arange(len(X_train), len(X_train) + len(X_test)))\n",
    "clf = find_best_parameters_quad(X, y, ExclusionSplitter(np.arange(0, len(X_train)), np.arange(len(X_train), len(X_train) + len(X_test))),\n",
    "                                [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3])\n",
    "#clf = find_best_parameters_quad2(X, y, ExclusionSplitter(np.arange(0, len(X_train)), np.arange(len(X_train), len(X_train) + len(X_test))),\n",
    "#                                [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3])\n",
    "\n",
    "clf = ClassifEstadisticoRegularizado(np.unique(y), [0.2, 0.21])\n",
    "clf.fit(X, y)\n",
    "#clf.decision_function(X)\n",
    "clf.num_aciertos(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
      "[0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3]\n"
     ]
    }
   ],
   "source": [
    "# 0.23 -> 0.956\n",
    "\n",
    "\n",
    "print(\"[\",end='')\n",
    "x = 10\n",
    "for ffff in range(x):\n",
    "    print(ffff/100, \", \", sep='', end='')\n",
    "print(x/100, end='')\n",
    "print(\"]\")\n",
    "\n",
    "print(\"[\",end='')\n",
    "x = 30\n",
    "for ffff in range(20, x):\n",
    "    print(ffff/100, \", \", sep='', end='')\n",
    "print(x/100, end='')\n",
    "print(\"]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "class prueba():\n",
    "    def __init__(self, labels, tremendo=0):\n",
    "        if tremendo == 0:\n",
    "            self.x = len(labels)\n",
    "        else:\n",
    "            self.x = tremendo\n",
    "    def print_X(self):\n",
    "        print(self.x)\n",
    "    \n",
    "p = prueba([1, 2, 3], tremendo=2)\n",
    "p.print_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Esqueleto(2).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
